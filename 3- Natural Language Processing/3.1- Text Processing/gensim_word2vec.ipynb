{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import *\n",
    "import spacy\n",
    "import nltk\n",
    "import gensim.downloader #pra trazer um modelo pre-pronto do google notícias pra trabalhar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "for model_name in list(gensim.downloader.info()[\"models\"].keys()):\n",
    "    print(model_name)\n",
    "\n",
    "#vamos usar o modelo word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chamando o gensim pra fazer o download do modelo certo\n",
    "g_news = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('joel', 0.48578500747680664), ('dubai', 0.4609808027744293), ('oliver', 0.45785653591156006), ('2Bdrm_suite', 0.45590725541114807), ('heidi', 0.4545367360115051), ('samuel', 0.4514700174331665), ('jh', 0.4510999917984009), ('florence', 0.44964927434921265), ('lohan', 0.44291403889656067), ('jessie', 0.44002825021743774)]\n"
     ]
    }
   ],
   "source": [
    "#aqui vamos procurar pela segunda palavra que mais se adequa a rome, que seria italy.\n",
    "#Ele nao achou, mas talvez fosse melhor colocar mais dados pra ele entender, ou letra maiuscula etc\n",
    "capital = g_news.most_similar([\"paris\", \"rome\"], [\"france\"])\n",
    "print(capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('marcus', 0.6085894703865051), ('brandon', 0.6059526205062866), ('jason', 0.6021084785461426), ('jeff', 0.60173100233078)]\n"
     ]
    }
   ],
   "source": [
    "#nesse caso estamos usando o word2vec pra ele achar as 4 palavras mais próximas de porsche\n",
    "w = g_news.most_similar(\"Porsche\", topn=4)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5555642\n"
     ]
    }
   ],
   "source": [
    "# numa escala de -1 a 1 ele acha o quanto essas palavras sao similares entre si. 0,55 é razoalvelmente alta\n",
    "c = g_news.similarity(\"Italy\", \"Rome\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agora vamos importar mais bibliotecas e fazer um processamento de dados\n",
    "import re\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pegando uma amostra de 20% pra nao ficar muito grande\n",
    "#Depois, resetando seu índice, pois a amostra bagunça\n",
    "\n",
    "movies_sample = df.sample(frac=0.2, replace=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Out to Sea was a great movie. I expected comed...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is it that a woman cannot be a strong char...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homeward Bound is a beautiful film. Y'know the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a small film , few characters ,theatri...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This film has not been seen by me in quite a f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>OK, what to say about Actium Maximus...&lt;br /&gt;&lt;...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Jolene (Heather Graham) operates a night club ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>It was an excellent piece to the puppet series...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>I don't know how anyone could hate this movie....</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>\"Down Periscope\" has been in our library since...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "0     Out to Sea was a great movie. I expected comed...  positive\n",
       "1     Why is it that a woman cannot be a strong char...  negative\n",
       "2     Homeward Bound is a beautiful film. Y'know the...  positive\n",
       "3     This is a small film , few characters ,theatri...  positive\n",
       "4     This film has not been seen by me in quite a f...  positive\n",
       "...                                                 ...       ...\n",
       "9995  OK, what to say about Actium Maximus...<br /><...  negative\n",
       "9996  Jolene (Heather Graham) operates a night club ...  positive\n",
       "9997  It was an excellent piece to the puppet series...  positive\n",
       "9998  I don't know how anyone could hate this movie....  positive\n",
       "9999  \"Down Periscope\" has been in our library since...  positive\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vou chamar as stopwords na minha lista sw e depois aplicarei a stematização (que seria cortar a palavra)\n",
    "#lembrando que eh a lematização que eu abrevio a palavra, mas a stematização eu corto pra um radical fixo\n",
    "sw_english = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string):\n",
    "    #Deixa apenas elementos alfanumericos\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", \" \", string)\n",
    "\n",
    "    #deixa as palavras minusculas\n",
    "    string = string.lower()\n",
    "\n",
    "    words = word_tokenize(string)\n",
    "\n",
    "    #removendo stopwords\n",
    "    filtered_words = [word for word in words if word not in sw_english]\n",
    "\n",
    "    #aplicando stematização\n",
    "    stem_words = [stemmer.stem(w) for w in filtered_words]\n",
    "\n",
    "    return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vou criar uma coluna no meu df chamada filtered_words cujo conteúdo será o texto original\n",
    "#após aplicação da função acima\n",
    "\n",
    "movies_sample[\"filtered_words\"] = movies_sample[\"review\"].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vou jogar o resultado numa lista\n",
    "words_list = movies_sample[\"filtered_words\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases, ENGLISH_CONNECTOR_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Os n_grams, no caso bi_gram, pois sao duas, pra analisar o contexto das palavras, afinal, word2vec pega a semântica das palavras, nao\n",
    "#só a frequencia, como com o tf-ifd. Então, como usarei bigrama, estarei analisando a relação entre 2 palavras, o contexto só entre elas duas.\n",
    "\n",
    "#entao, eu vou passar a lista criada acima, as conexões terão que ser com palavras cuja frequencia mínima seja 1, utilizando os\n",
    "#conectores, pois as vezes, tirar o ON, IN, OF etc entre 2 palavras pode tirar o sentido.\n",
    "phrases = Phrases(words_list, min_count=1, connector_words=ENGLISH_CONNECTOR_WORDS, threshold=1)\n",
    "\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "sentences = list(bigram[words_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feel_good', 'movi_noth', 'great_fun', 'watch', 'sure', 'skim', 'polit_issu', 'sure', 'tri_make', 'good_night', 'good_luck', 'let_tri', 'make', 'anyth_els', 'light_fare', 'br_br', 'enjoy', 'br_br', 'rememb', '1984', 'like', 'becom', 'sophist', 'accord', 'media', 'far', 'watch', 'tend_differ', 'point', 'goldi', 'knew', 'fun', 'fluff', 'went_ahead', 'br_br', 'like', 'lightest', 'fare', 'protocol', 'overboard', 'housesitt', 'wildcat', 'privat_benjamin', 'seem_like', 'old_time', 'foul', 'play', 'death', 'becom', 'first_wive', 'club', 'remak', 'towner', 'goldi_know', 'play', 'everi_role', 'camp', 'get', 'goldi_know', 'realli_well', 'know', 'realli_well', 'br_br', 'alway', 'made_laugh', 'cheer', 'innoc', 'love', 'laugh_everi', 'thing_ever', 'never_tri', 'anyth_els', 'bubbl', 'giggli', 'girl_next', 'door', 'happen', 'pretti_smile', 'laugh', 'alway', 'endear', 'remind', 'life', 'pretti_short', 'got', 'lighten_know', 'old', 'wrinkl', 'suffer', 'one', 'life', 'inevit', 'ailment', 'even_come', 'late', 'br_br', 'appreci', 'goldi', 'lovabl', 'comic', 'actress']\n"
     ]
    }
   ],
   "source": [
    "#peguei uma sentença aqui pra poder analisar o que aconteceu\n",
    "print(sentences[3705])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A partir disso, a gente chama o word2vec, passa a lista de palavras acima, cujo minimo de aparições seja 2,\n",
    "# rodando em 2 cpu diferente pra ficar mais rapido\n",
    "model = Word2Vec(words_list, min_count=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aw', 0.8556991815567017),\n",
       " ('suck', 0.8493819236755371),\n",
       " ('terribl', 0.8444787859916687),\n",
       " ('horribl', 0.8284021615982056),\n",
       " ('ok', 0.8104258179664612),\n",
       " ('dumb', 0.78822261095047),\n",
       " ('lame', 0.7741866707801819),\n",
       " ('pathet', 0.771503210067749),\n",
       " ('stupid', 0.7707765698432922),\n",
       " ('damn', 0.7640008926391602)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vou aplicar aquele most_similar la de cima pra palavra bad, que me trará as principais palavras mais similares ao bad\n",
    "model.wv.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acredito que o professor tenha parado aqui pois já criou um modelo similar ao g_news, pra buscar similaridade entre as palavras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
