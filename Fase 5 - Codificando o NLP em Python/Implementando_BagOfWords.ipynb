{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import heapq #pra fazer contagem de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Artificial_neural_network\")\n",
    "raw_html = raw_html.read()\n",
    "\n",
    "article_html = bs.BeautifulSoup(raw_html, \"lxml\") #vou aplicar o tipo de leitura xml do beautifulsoup\n",
    "article_paragraphs = article_html.find_all(\"p\") #procurar as tags do tipo \"p\"\n",
    "article_text = \"\" #uma variável por enquanto vazia\n",
    "\n",
    "for par in article_paragraphs:  #vou adicionando cada p que eu achar nessa variavel vazia\n",
    "    article_text += par.text\n",
    "\n",
    "corpus = nltk.sent_tokenize(article_text) #aqui eu pego a variavel que ta com 1 string com todas\n",
    "                                          #os paragrafos e separo ela por frases em uma lista.\n",
    "\n",
    "for i in range(len(corpus)): #vou aplicar em cada frase a transformacao pra minuscula\n",
    "    corpus[i] = corpus[i].lower()\n",
    "    corpus[i] = re.sub(r\"\\W\", \" \", corpus[i]) #\\w significa caracter de palavra. \\W signifca o que nao é caracter\n",
    "                                              #de palavra. Logo, eu estou substituindo na frase tudo o que nao\n",
    "                                              #é letra por espaço\n",
    "    corpus[i] = re.sub(r\"\\s+\", \" \", corpus[i]) #retirar um ou mais espaços em branco\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they do this without any prior knowledge of cats for example that they have fur tails whiskers and cat like faces \n"
     ]
    }
   ],
   "source": [
    "print(corpus[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando um dicionário contendo a frequencia das minhas palavras. Ou seja, {they:2} no caso acima.\n",
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvando na minha variavel as 200 palavras que com maior frequencia\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando uma lista de 1 e 0 da minha matriz esparsa.\n",
    "#Entao vou criar uma coluna pra cada palavra e marcar, em cada linha, ou seja, pra cada frase, o 1\n",
    "#onde aparecer aquela determinada palavra e o 0 pras que não estavam presentes nessa frase\n",
    "#Nao estou me atendo a contagem das palavras, mas ao simples de fato \"ou tem ou nao tem (0ou1)\"\n",
    "#Entao vou transformar isso num vetor e ver como fica\n",
    "\n",
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = np.asarray(sentence_vectors) #agora, finalmente criando um array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isso foi pra entender a base de um bag of words. Mas na pratica, eh so continuar utilizando o spacy ou nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
